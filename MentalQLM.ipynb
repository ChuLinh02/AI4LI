{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOb8xQjxvlP+EskcdBbZEyx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChuLinh02/AI4LI/blob/main/MentalQLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import librabries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.stats import zscore"
      ],
      "metadata": {
        "id": "wthZVK9Br--3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAYI71i-plHI",
        "outputId": "c814e8fb-b97d-4df9-be19-2d9acb45279e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSelector:\n",
        "    def __init__(self, model_name: str, data_path: str, output_path: str, k: int, z_score_threshold: float):\n",
        "        self.model_name = model_name\n",
        "        self.data_path = data_path\n",
        "        self.output_path = output_path\n",
        "        self.k = k\n",
        "        self.z_score_threshold = z_score_threshold\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id  # Ensure padding token is set\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.data = self.load_data()\n",
        "        self.perplexities = None\n",
        "        self.filtered_data = None\n",
        "        self.vector_embeddings = None\n",
        "\n",
        "    def load_data(self):\n",
        "        df = pd.read_csv(self.data_path)\n",
        "        return list(zip(df['post'], df['response']))\n",
        "\n",
        "    def compute_perplexity(self, instruction: str, output: str) -> float:\n",
        "        text = f\"{instruction} {output}\"\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            padding=True  # Ensure uniform input length\n",
        "        )\n",
        "\n",
        "        input_ids = encodings.input_ids\n",
        "        attention_mask = encodings.attention_mask\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        shift_logits = logits[:, :-1, :].contiguous()\n",
        "        shift_labels = input_ids[:, 1:].contiguous()\n",
        "\n",
        "        # Mask out padding tokens\n",
        "        valid_indices = shift_labels != self.tokenizer.pad_token_id\n",
        "\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),  # Reshape logits\n",
        "            shift_labels.view(-1)  # Reshape labels\n",
        "        )\n",
        "\n",
        "        return torch.exp(loss).item()\n",
        "\n",
        "    def filter_data(self):\n",
        "        self.perplexities = np.array([self.compute_perplexity(x, y) for x, y in self.data])\n",
        "        filtered_indices = np.where(np.abs(zscore(self.perplexities)) < self.z_score_threshold)[0]\n",
        "        self.filtered_data = [self.data[i] for i in filtered_indices]\n",
        "\n",
        "    def vectorize_data(self):\n",
        "        tfidf = TfidfVectorizer()\n",
        "        text_data = [\" \".join(pair) for pair in self.filtered_data]\n",
        "        self.vector_embeddings = tfidf.fit_transform(text_data).toarray()\n",
        "\n",
        "    def k_center_greedy(self) -> list:\n",
        "        first_index = np.argmin(np.abs(self.perplexities - np.mean(self.perplexities)))\n",
        "        selected_indices = [first_index]\n",
        "        for _ in range(self.k - 1):\n",
        "            remaining = np.delete(self.vector_embeddings, selected_indices, axis=0)\n",
        "            distances = cdist(self.vector_embeddings[selected_indices], remaining, metric=\"euclidean\").min(axis=0)\n",
        "            new_center = np.argmax(distances)\n",
        "            selected_indices.append(new_center)\n",
        "        return selected_indices\n",
        "\n",
        "    def select_and_save_data(self):\n",
        "        self.filter_data()\n",
        "        self.vectorize_data()\n",
        "        selected_indices = self.k_center_greedy()\n",
        "        final_selected_data = [self.filtered_data[i] for i in selected_indices]\n",
        "        selected_df = pd.DataFrame(final_selected_data, columns=['query', 'gpt-3.5-turbo'])\n",
        "        selected_df.to_csv(self.output_path, index=False)\n",
        "        print(f\"Selected data saved to {self.output_path}\")"
      ],
      "metadata": {
        "id": "iKusxYWHCGOJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "selector = DataSelector(\n",
        "      model_name=\"gpt2\",\n",
        "      data_path=\"/content/gdrive/MyDrive/AI4LI/IMHI dataset/train_data/complete_data/DR/reddit_train.csv\",\n",
        "      output_path=\"selected_reddit_train_data.csv\",\n",
        "      k=10,\n",
        "      z_score_threshold=2\n",
        "  )\n",
        "\n",
        "selector.select_and_save_data()"
      ],
      "metadata": {
        "id": "bzVScYV2CUYv",
        "outputId": "1e5c365c-4592-4005-99d1-304c0b59e693",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected data saved to selected_reddit_train_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "selector = DataSelector(\n",
        "      model_name=\"gpt2\",\n",
        "      data_path=\"/content/gdrive/MyDrive/AI4LI/IMHI dataset/train_data/complete_data/dreaddit/dreaddit-train.csv\",\n",
        "      output_path=\"selected_dreaddit_train_data.csv\",\n",
        "      k=10,\n",
        "      z_score_threshold=2\n",
        "  )\n",
        "\n",
        "selector.select_and_save_data()"
      ],
      "metadata": {
        "id": "LSLoxtkYG3T_",
        "outputId": "9fdec4f5-40c8-423f-929b-d726472f2253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected data saved to selected_dreaddit_train_data.csv\n"
          ]
        }
      ]
    }
  ]
}